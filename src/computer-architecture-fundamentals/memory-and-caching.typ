== Interlude: Memory, Caches, and Consistency

For a processor to perform well, it needs relatively fast access to memory.
Main memory technologies like _dynamic random access memory_ (DRAM) have high capacities and generally high bandwidths, which is good, but they also have high _latencies_; the time between requesting data and getting the data back.
Even though superscalar processors can finish more than one instruction per cycle, this is only in theory.

A good number of instructions within any given program are likely to be memory instructions.
Instructions are generally read from memory.
Because of this, the effective latency of memory accesses has a huge impact on actual processor performance.

=== Caches

To reduce latency in accessing main memory, the memory can be placed closer to the core, physically on the same chip.
However, this is costly and scales poorly for large capacities.
The compromise is to use _caches_ alongside main memory technologies like DRAM.

Caches, as a concept, show up everywhere in computing.
Generally, they are used to store temporary copies of frequently or recently used data in some sort of storage that is faster to access than the main storage.
The cached data must eventually be synchronised with the main storage, and somehow propagated to other copies, giving rise to one of the two hard problems in computer science @bib:two-hard-things.

Computer processor caches base themselves on this same idea.
Small low-latency memories are placed close to the core and are made to store copies of the data in main memory.
This works because applications generally exhibit _locality_ in memory requests.
Data that have been used previously are more likely to be re-used---called _temporal locality_.
Programs also exhibit _spatial locality_: data that lie close to previously used data are more likely to be requested in the future.
Computer processor caches exploit this by not only fetching the requested datum, but surrounding data as well.

=== Cache Hierarchies

Caches are arranged in hierarchies, with the smallest, fastest caches placed closest to the core while the larger, slower caches are placed further away---sometimes on a different chip entirely.
If the processor requests data that cannot be found in the first level of cache, the data will be looked up in lower and lower levels until it is found, possibly all the way out in main memory.
The first level of cache is generally called the L1 cache, or level-1 cache.
Each consecutive level is simply numbered one higher than the last.
Modern processors generally have first-, second-, and third-level caches.
The last level of cache is appropriately named the last-level cache (LLC).

=== Split L1 Caches

Generally, all instructions have to be fetched from memory, and some instructions have to access memory.
These two sources of memory requests---instruction fetch and memory access---have very different patterns and behaviours:
The stream of requests generated by instruction fetch is very predictable and linear except for where branching occurs, often for one specific size, and is read-only.
The instruction fetching circuitry will never try to write an instruction to memory.

On the other hand, the stream of requests generated by memory instructions is going to hop back and forth between many different memory locations, may contain store instructions, and access many different sizes of data.

To avoid designing a cache that is good for both types of requests, the first-level cache is usually split into a first-level data cache (L1d), and a first-level instruction cache (L1i).
This allows implementers to focus on optimising each cache for specific needs.
The instruction cache can be optimised for reading consecutive instructions out at a high bandwidth.
The data cache can be optimised for accessing differently sized elements at different locations each cycle.

=== Cache Organisation

Caches are organised as a collection of _sets_.
When accessing the cache, bits from the address are used to select the specific set for the processor to look in.
These bits form the _index_.
This means that the data of any given address can only be found within a specific set.
Each set is also split into one or more _blocks_ or _lines_.
We will refer to them as blocks.
Each block contains a number of bytes, usually 64, which are the actual data being accessed.
The lower 6 bits of the address are used to determine which specific bytes of the block are being accessed.
This is called the block _offset_.
The bits of an address that are not part of the index and offset are part of the _tag_.
Each block has a tag.

The number of blocks within sets is called the _associativity_.
With one block per set, the cache is a _direct-mapped_ cache (each datum has only one possible cache block it can be in).
With only one set and all blocks placed within it, the cache is _fully associative_ (each datum can be in any block in the cache).

Fully associative caches are expensive to implement as it requires comparing the address tag with the tag of every block in the cache.
Direct-mapped caches are simple to implement, but have problems with aliasing where multiple addresses with equal indices, but different tags, will cause each other to be pushed out of the cache to make space for themselves.
The compromise is an _n-way_ associative cache, where $n$ is the number of blocks within each set.
These caches can store up to $n$ blocks with equal indices but different tags at the same time without pushing any blocks out to make space.

When a request is made and the cache does not have the requested datum, the request is said to _miss_ in the cache.
The converse of a miss is a _hit_ in the cache.
When a miss occurs, the entire block that contains that datum is brought in along with surrounding data.
Using larger blocks reduces the amount of meta-data that must be stored per byte, although it requires fetching more data at a time when first missing.

=== Data Cache Prefetching

Naively, data are only brought into caches on-demand.
If no previous memory request has been made to any address within a block, a request to that block will always miss.
However, the access patterns of memory instructions are predictable.
For example: iterating over an array in a loop will generate requests for addresses that increment by some fixed amount.
Thus, if one such request starts by accessing the second-to-last byte in a block, then the last one, it is reasonable to assume that the next request will be for the first byte in the next block.
Using this information, the cache can begin loading the block before it is even known to be needed---called _prefetching_.

==== Evaluating Data Cache Prefetching

Several schemes have been invented to predict which blocks of data are likely to be needed in the future and prefetch them in time for the next request.
Various metrics affect the efficacy of cache prefetching such as _accuracy_, _coverage_, and _timeliness_.
Accuracy is the number of useful prefetches compared to the number of performed prefetches.
Coverage is the fraction of misses that instead become hits due to prefetching.
Timeliness is a metric that says something about the usefulness of the data that are prefetched.
If a prefetch request goes out too late, the next request may come in before the data are fully in cache.

Timeliness is more of a yes-or-no answer, while accuracy and coverage are metrics with tradeoffs.
Issuing more prefetches when the prediction is uncertain but there is available capacity may increase the coverage at the cost of reducing accuracy and likely wasting more power.
Issuing more prefetches might also have the effect of reducing performance by taking up capacity that could be used to fetch data that are known to be needed, or it might cause useful data to be _evicted_ (pushed out) from the cache to make space for prefetched data that aren't useful.

=== Memory Consistency Models and Cache Coherency Protocols

ISA documents generally do not specify how a cache should behave.
